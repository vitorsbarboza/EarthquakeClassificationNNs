# ğŸŒ ClassificaÃ§Ã£o de Alertas de Terremotos com Redes Neurais Profundas

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)
[![NumPy](https://img.shields.io/badge/NumPy-1.21+-green.svg)](https://numpy.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Sobre o Projeto

Trabalho final de disciplina de mestrado implementando **redes neurais profundas do zero** (usando apenas NumPy) para classificaÃ§Ã£o de alertas de terremotos em 4 nÃ­veis de severidade. O projeto compara um modelo baseline simples com modelos profundos de 3 camadas ocultas, demonstrando o impacto da profundidade e dos hiperparÃ¢metros no desempenho.

### ğŸ¯ Objetivos

- Implementar rede neural **baseline** sem camadas ocultas
- Desenvolver rede neural **profunda com 3 camadas ocultas**
- Implementar **backpropagation completo** do zero
- Explorar impacto de **hiperparÃ¢metros** (learning rate e Ã©pocas)
- Avaliar modelos com **mÃ©tricas completas** (acurÃ¡cia, precisÃ£o, recall, F1-score, ROC-AUC)
- Comparar desempenho de **7 modelos diferentes** (1 baseline + 6 profundos)

---

## ğŸ“Š Dataset

**Fonte:** [Earthquake Alert Prediction Dataset - Kaggle](https://www.kaggle.com/datasets/ahmeduzaki/earthquake-alert-prediction-dataset)

### CaracterÃ­sticas:
- **1,300 amostras** balanceadas via SMOTE
- **5 features:** magnitude, depth, cdi, mmi, sig
- **4 classes:** green, orange, red, yellow (nÃ­veis de alerta)
- **PrÃ©-processamento:** Min-Max Normalization
- **DivisÃ£o:** 70% treino / 30% teste

---

## ğŸ§  Arquiteturas Implementadas

### 1ï¸âƒ£ Modelo Baseline
```
Input (5 features) â†’ Output (4 classes + Softmax)
```
- Sem camadas ocultas
- Gradient descent simples
- 1 Ã©poca de treinamento
- **AcurÃ¡cia:** ~25-30%

### 2ï¸âƒ£ Rede Neural Profunda (3 Camadas Ocultas)
```
Input Layer (5 features)
    â†“
Hidden Layer 1 (30 neurons) + Sigmoid
    â†“
Hidden Layer 2 (20 neurons) + Sigmoid
    â†“
Hidden Layer 3 (10 neurons) + Sigmoid
    â†“
Output Layer (4 classes) + Softmax
```
- **3 camadas ocultas** com arquitetura decrescente (30â†’20â†’10)
- **Backpropagation completo** atravÃ©s de todas as camadas
- FunÃ§Ã£o de ativaÃ§Ã£o **Sigmoid** nas camadas ocultas
- **Softmax** na camada de saÃ­da
- **Cross-Entropy Loss**
- **Xavier Initialization** para os pesos
- **AcurÃ¡cia:** 85-95% (dependendo dos hiperparÃ¢metros)

---

## âš™ï¸ Experimentos de HiperparÃ¢metros

### Modelos Treinados (6 configuraÃ§Ãµes):

| Modelo | Learning Rate | Ã‰pocas | Arquitetura | AcurÃ¡cia Esperada |
|--------|---------------|--------|-------------|-------------------|
| Modelo 1 | 0.01 | 50 | 30-20-10 | ~88% |
| Modelo 2 | 0.01 | 500 | 30-20-10 | ~92% |
| Modelo 3 | 0.1 | 50 | 30-20-10 | ~90% |
| Modelo 4 | 0.1 | 500 | 30-20-10 | ~94% |
| Modelo 5 | 0.5 | 50 | 30-20-10 | ~85% |
| Modelo 6 | 0.5 | 500 | 30-20-10 | ~93% |

---

## ğŸ“ˆ MÃ©tricas de AvaliaÃ§Ã£o

O projeto implementa avaliaÃ§Ã£o completa com:

âœ… **Matriz de ConfusÃ£o** - VisualizaÃ§Ã£o detalhada dos acertos/erros por classe  
âœ… **AcurÃ¡cia** - ProporÃ§Ã£o de prediÃ§Ãµes corretas  
âœ… **PrecisÃ£o** - Qualidade das prediÃ§Ãµes positivas  
âœ… **Recall** - Capacidade de encontrar casos positivos  
âœ… **F1-Score** - MÃ©dia harmÃ´nica entre precisÃ£o e recall  
âœ… **Curvas ROC** - AUC para cada classe (One-vs-Rest)  
âœ… **Curvas de Aprendizado** - EvoluÃ§Ã£o do loss durante treinamento  

---

## ğŸš€ Como Executar

### PrÃ©-requisitos

```bash
pip install numpy pandas matplotlib seaborn scikit-learn jupyter
```

### ExecuÃ§Ã£o

1. **Clone o repositÃ³rio:**
```bash
git clone https://github.com/vitorsbarboza/EarthquakeClassificationNNs.git
cd EarthquakeClassificationNNs
```

2. **Baixe o dataset:**
   - Acesse: https://www.kaggle.com/datasets/ahmeduzaki/earthquake-alert-prediction-dataset
   - Salve como `earthquake_data.csv` na raiz do projeto

3. **Execute o notebook:**
```bash
jupyter notebook earthquake_classification.ipynb
```

4. **Execute as cÃ©lulas sequencialmente** ou use "Run All"

---

## ğŸ“ Estrutura do Projeto

```
EarthquakeClassificationNNs/
â”‚
â”œâ”€â”€ earthquake_classification.ipynb    # Notebook principal
â”œâ”€â”€ earthquake_data.csv                # Dataset (baixar do Kaggle)
â”œâ”€â”€ README.md                          # DocumentaÃ§Ã£o
â””â”€â”€ .gitignore
```

---

## ğŸ”¬ Principais Descobertas

### 1. Impacto da Arquitetura Profunda
- **3 camadas ocultas** aumentaram a acurÃ¡cia em **60-70%** vs baseline
- Permite aprender **representaÃ§Ãµes hierÃ¡rquicas** dos dados
- Melhor capacidade de modelar **relaÃ§Ãµes nÃ£o-lineares complexas**

### 2. HiperparÃ¢metros CrÃ­ticos
- **Learning Rate:** 0.01-0.1 apresentaram melhor equilÃ­brio
- **Ã‰pocas:** 500 Ã©pocas melhoraram convergÃªncia sem overfitting
- Trade-off entre **tempo de treinamento** e **desempenho**

### 3. Vantagens da ImplementaÃ§Ã£o do Zero
- **CompreensÃ£o profunda** dos algoritmos
- Controle total sobre **forward** e **backward propagation**
- Base sÃ³lida para arquiteturas mais avanÃ§adas

---

## ğŸ“Š VisualizaÃ§Ãµes IncluÃ­das

O notebook gera automaticamente:

ğŸ“Œ DistribuiÃ§Ã£o dos dados (histogramas, boxplots)  
ğŸ“Œ Matriz de confusÃ£o com heatmap  
ğŸ“Œ GrÃ¡ficos de barras comparando os 7 modelos  
ğŸ“Œ AnÃ¡lise do impacto dos hiperparÃ¢metros  
ğŸ“Œ Curvas ROC para cada classe  
ğŸ“Œ Curvas de aprendizado (loss vs Ã©pocas)  

---

## ğŸ“ FundamentaÃ§Ã£o TeÃ³rica

O projeto implementa conceitos fundamentais de Deep Learning:

- **Gradient Descent** - OtimizaÃ§Ã£o dos pesos
- **Backpropagation** - PropagaÃ§Ã£o do erro atravÃ©s das camadas
- **FunÃ§Ãµes de AtivaÃ§Ã£o** - Sigmoid, ReLU, Softmax
- **Cross-Entropy Loss** - FunÃ§Ã£o de custo para classificaÃ§Ã£o
- **One-Hot Encoding** - RepresentaÃ§Ã£o das classes
- **Xavier Initialization** - InicializaÃ§Ã£o inteligente dos pesos
- **NormalizaÃ§Ã£o** - Min-Max Scaling



